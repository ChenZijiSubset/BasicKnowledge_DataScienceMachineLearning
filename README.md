# ğŸŒ Data Science and Machine Learning for Planet Earth - Lectures

## ğŸ“š Course overview and learning outcomes 

Welcome to the <strong style = "color:teal">Data Science and Machine Learning for Planet Earth</strong> module. This module will introduce the concepts central to the data science approach, including working in a computational notebook environment, data preparation and data pipelines, and different types of classical machine learning algorithms for regression, classification and clustering.

This module's learning outcomes are:
1.	Describe the different steps involved in a data science / machine learning project, from initial data exploration to production code. 
2.	Clean and manipulate different types of data and prepare them for machine learning analysis using open source libraries. 
3.	Select an appropriate machine learning algorithm for the problem at hand and  optimize the predictions by tuning hyperparameters 
4.	Test the performance of a machine learning pipeline through various performance metrics 

Below, you can access the lecture notes and the plan for each day of the two weeks module.

## Day 1: Data Preprocessing ğŸ§¹

<a href="lectures/01-Data-Preparation.ipynd">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* The landscape of Machine Learning
* Introduction to Scikit-Learn
* Overview of Data Preprocessing
* Concept of Generalization
* Parametric vs non-parametric models

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Introduction to Data Science 
* Consolidate your NumPy and Pandas skills 
* Confidently preprocess data for analysis 
* Basic use of the Scikit-learn Library 

## Day 2: Performance metrics ğŸ“

<a href="lectures/02-Evaluation-Metrics.md">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Baseline score
* Regression Metrics
* Classification Metrics
* ROC-AUC

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Confidently choose the right evaluation metric
* Compare different model performance
* Use different metrics in CrossValidation
* Use and tune RandomForest for classification and regression

## Day 3: Optimization ğŸ•µï¸

<a href="lectures/03-ML-Optimization.ipynb">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Understanding what fitting a model means
* Brief introduction to Gradient Descent and other solvers
* Regression loss Functions
* Logistic function and cross entropy cost

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Understand weights and biases
* Build an intuition for ML solvers
* Choose an appropriate loss function

## Day 4: Model tuning ğŸµ

<a href="lectures/04-Model-Tuning.ipynb">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Bias/Variance tradeoff
* Learning Curve
* Model Selection
* Regularizing models
* Model Tuning
* Support Vector Machine

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Choose a fit for purpose model
* Regularize to avoid overfitting
* Use SVR and SVC for respectively regression and classification

## Day 5: ML Workflow ğŸ‘·

<a href="lectures/05-ML-Workflow.ipynb">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Motivation for an ML workflow
* Introduction to <code>sklearn.pipeline</code> module
* Writing custom transformers
* Grouping data transformation and models into one object
* Migrating from Notebooks to Python classes

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Write clean code using pipelines
* Optimize the entire **data-preparation-to-model-selection** chain
* Build code deployable locally and on the cloud 

## Day 6: Ensemble Learning ğŸ«

<a href="lectures/06-Ensemble-Methods.ipynb">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* <code>DecisionTree</code> for classification
* <code>DecisionTree</code> for regression
* Bagging algorithms: <code>RandomForest</code>
* Boosting: <code>AdaBoost</code> and <code>xgboost</code>
* Stacking algorithms

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Understand variance in decision trees
* Apply RandomForest and xgboost (some of the most powerful algorithms
* Unleash the power of Ensemble methods on your problem 

## Day 7: Unsupervised Learning ğŸ‘»

<a href="lectures/07-Unsupervised-Learning.ipynb">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Dimensionality reduction with matrix factorization (<code>PCA</code>)
* Dimensionality reduction with graph algorithms (</code>tSNE<code>, <code>UMAP</code>)
* Clustering algorithms (<code>KMeans</code>, <code>DBSCAN</code>, <code>HDBSCAN</code>)
* Anomaly detection (<code>IFOR</code>, <code>OCSVM</code>)

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Build a feel for what unsupervised machine learning can do
* Apply dimensionality reduction to data visualization
* Explore dataset by clustering similar observations
* Identify potential outliers 

## Day 8: Geospatial Data and Time Series ğŸŒ

<a href="lectures/08-Dealing-with-Spatial-and-Time-data.ipynb">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Spatial and temporal covariance
* Using Machine Learning to predict spatial and temporal data
* Semivariograms and geostatistics
* Geospacial interpolation with Kriging

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Be able to choose the correct train-test split method
* Handling non-stationarity for machine learning modelling
* Be able to extrapolate spatial data with Kriging 

## Day 9: An Introduction to Neural Networks ğŸ•¸ï¸

<a href="lectures/09-Neural-Networks.md">See lecture notes here</a>

ğŸ“† ***Lecture Plan:***
* Introduction to <code>TensorFlow.keras</code>
* Overview of the building blocks of neural networks
* Compiling and training neural networks
* Deep-Learning: adapting neural network architectures to specific tasks

ğŸ‘©â€ğŸ“ ***Intended Learning Outcomes:***
* Be comfortable using <code>TensorFlow.keras</code>
* Correctly select the type of neural layer for your task
* Compile, train and assess neural networks
* Gain confidence in the method in preparation for your Deep-Learning module 

## Day 10: Q&A and assessed timed coursework ğŸ§ª

We will use Friday morning for self-study, and for any Q&A you might have. Starting at 1 pm, you will take a timed assessed coursework.
