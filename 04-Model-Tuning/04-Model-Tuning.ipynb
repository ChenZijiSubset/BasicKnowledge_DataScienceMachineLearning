{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2153c3d8",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef6c8d",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:maroon\">Model Tuning</h1>\n",
    "    <img src=\"figures/04-model_tuning.jpeg\" style=\"width:1300px;\">\n",
    "    <h3><span style=\"color: #045F5F\">Data Science & Machine Learning for Planet Earth Lecture Series</span></h3><h6><i> by C√©dric M. John <span style=\"size:6pts\">(2022)</span></i></h6></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502b84a",
   "metadata": {},
   "source": [
    "## Plan for today's Lecture üóì "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875deaf4",
   "metadata": {},
   "source": [
    "* Bias/Variance tradeoff\n",
    "* Learning Curve\n",
    "* Model Selection\n",
    "* Regularizing models\n",
    "* Model Tuning\n",
    "* Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c297375",
   "metadata": {},
   "source": [
    "## Intended learning outcomes üë©‚Äçüéì"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430f46b",
   "metadata": {},
   "source": [
    "* Choose a fit for purpose model\n",
    "* Regularize to avoid overfitting\n",
    "* Use SVR and SVC for respectively regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfaa31",
   "metadata": {},
   "source": [
    "# \"No Free Lunch\" Theorem\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_freelunch.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>Photo of a beautifully colored greek lunch with olives, mousaka and various other elements set on a white and blue checkered table cloth, sunny bright lighting</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ef609",
   "metadata": {},
   "source": [
    "# Data used today\n",
    "We are back to the <a href=\"https://www.kaggle.com/aungpyaeap/fish-market\">Kaggle fish market dataset</a>.\n",
    "<img src=\"figures/fish_dataset.png\" style=\"width:1300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf1ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some packages we will use later to plot...\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('data/fish_no_pikes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90559a",
   "metadata": {},
   "source": [
    "#  Bias vs Variance \n",
    "<img src=\"figures/variance_tradeoff_bullseye.png\" style=\"width:900px;\">\n",
    "<a href=\"http://scott.fortmann-roe.com/docs/BiasVariance.html\">Scott-Fortmann, 2012</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4218f",
   "metadata": {},
   "source": [
    "# The Bias / Variance tradeoff\n",
    "For a model to generalize there will be a tradeoff between **bias** and **variance**.\n",
    "<img src=\"figures/over_under_bias_variance.png\" style=\"width:1300px\">\n",
    "<a href=\"https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\">Singh, 2018</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1447ddf9",
   "metadata": {},
   "source": [
    "* **Bias (Underfitting)**: The inability for an algorithm to learn the patterns within a dataset.\n",
    "* **Variance (Overfitting)**: The algorithm generates an overly complex relationship when modelling patterns within a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a82e5",
   "metadata": {},
   "source": [
    "## üêü Testing with our fish dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16559014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import polynomial_regression\n",
    "polynomial_regression(data=df,degrees=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ec983d",
   "metadata": {},
   "source": [
    "## No Free Lunch Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02fca1b",
   "metadata": {},
   "source": [
    "Some models **oversimplify**, while others **overcomplicate** a relationship between features and target.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e259b",
   "metadata": {},
   "source": [
    "It's up to us data scientists to make **assumptions** about the data and evaluate reasonable models accordingly.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34558fe2",
   "metadata": {},
   "source": [
    "**There is no one size fits all model**, this is known as the **No Free Lunch Theorem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39e02a9",
   "metadata": {},
   "source": [
    "## The Learning Curves\n",
    "We can use learning curves to diagnose three aspects of model behaviour on the dataset:\n",
    "* Underfitting\n",
    "* Overfitting\n",
    "* Whether the model has sufficient data to learn the patterns of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69c6bd",
   "metadata": {},
   "source": [
    "### Concept\n",
    "Increasing the size of the training set can affect the training and validation scores.\n",
    "![lc](figures/learning_curves_transposed.png)\n",
    "<p><a href=\"https://www.dataquest.io/blog/learning-curves-machine-learning/\">Olteanu, 2018</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93a09a",
   "metadata": {},
   "source": [
    "### Reading  the curves\n",
    "As the training size increases:\n",
    "* The training score will decrease\n",
    "* The testing score will increase\n",
    "* The curves typically (but not always!) demonstrate convergence\n",
    "![lc1](figures/learning_curve_1.png)\n",
    "<p><a href=\"https://www.dataquest.io/blog/learning-curves-machine-learning/\">Olteanu, 2018</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c31771",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_train, lc_test = train_test_split(df, train_size=.8, random_state=3)\n",
    "val_score = []\n",
    "train_score = []\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,8))\n",
    "for nb_samples in range(1,100):\n",
    "    data = lc_train.iloc[:nb_samples]\n",
    "    poly_tr = PolynomialFeatures(degree=2).fit(data.drop(columns=['Weight', 'Species']))\n",
    "    poly = poly_tr.transform(data.drop(columns=['Weight', 'Species']))\n",
    "    lin_model = LinearRegression().fit(poly, data.Weight)\n",
    "    y_pred = lin_model.predict(poly)\n",
    "    y_test_pred = lin_model.predict(poly_tr.transform(lc_test.drop(columns=['Weight', 'Species'])))\n",
    "\n",
    "    train_score.append(np.sqrt(mean_squared_error(data.Weight, y_pred)))\n",
    "    val_score.append(np.sqrt(mean_squared_error(lc_test.Weight, y_test_pred)))\n",
    "                     \n",
    "ax.plot(train_score, label='Training RMSE')\n",
    "ax.plot(val_score, label='Testing RMSE')\n",
    "ax.set_xlabel('Number of training samples', size = 14)\n",
    "ax.set_ylabel('RMSE', size = 14)\n",
    "ax.legend();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc160e",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"The-Bias-Variance-Tradeoff\">The Bias-Variance Tradeoff</a></h3><p>One of the most important concepts in Data science!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d1a6d",
   "metadata": {},
   "source": [
    " Measuring the error on an unseen **Test set**:<br>\n",
    "<img src=\"figures/biasvariancetradeoff.png\" style=\"width:1300px;\">\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
    "üìö <a href=\"https://hastie.su.domains/ElemStatLearn/\">Hastie et al, 2009 (Elements of Statistical Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05128a",
   "metadata": {},
   "source": [
    "Best model complexity is the one reducing the **Total Error** on a unseen dataset\n",
    "<img src=\"figures/model_complexity_error_training_test.jpg\" style=\"width:1300px;\">\n",
    "üìö <a href=\"https://hastie.su.domains/ElemStatLearn/\">Hastie et al, 2009 (Elements of Statistical Learning)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c9f905",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_selection.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A large wooden cheeseboard with five types of cheese, various small decorative flowers and tomatoes, gurken, pickled onions, and a cheese knife with a</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea6909",
   "metadata": {},
   "source": [
    "# What data to use to select the right model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2894f",
   "metadata": {},
   "source": [
    "* Do NOT use the **test set**: would lead to overfitting\n",
    "* Instead use a *Validation Set*\n",
    "<p><img alt=\"image.png\" src=\"data:image/png;base64,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\"/></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c96f088",
   "metadata": {},
   "source": [
    "\n",
    "* Even better: <strong>cross-validate</strong> instead of a using single holdout val set cross validate with the train set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79c65c",
   "metadata": {},
   "source": [
    "### Model complexity vs dataset size (rule of thumb üëç)\n",
    "\n",
    "* More than 100,000 datapoints: Parametric models (SGD, Neural Nets)\n",
    "* Less than 100,000 datapoints: Non-parametric models (KNN, SVM, Decision Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6495e64",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"figures/ml_cheat_sheet.png\" style=\"width:1500px;\">\n",
    "<a href=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\"><code>sklearn</code> algorithm cheat sheet</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d62f209",
   "metadata": {},
   "source": [
    "### What to do if the model overfits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ce7380",
   "metadata": {},
   "source": [
    "\n",
    "<blockquote><p><em>Simplify</em> your model <em>relatively</em> to your data</p>\n",
    "</blockquote>\n",
    "<ul>\n",
    "<li>Choose a simpler model</li>\n",
    "<li>Get more observations</li>\n",
    "<li>Feature selection (manual or <a href=\"https://scikit-learn.org/stable/modules/feature_selection.html\">automated</a>)</li>\n",
    "<li>Dimensionality reduction (Unsupervised Learning)</li>\n",
    "<li>Early stopping (Deep Learning)</li>\n",
    "<li><strong>Regularization</strong> of your Loss function</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14821e8",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc30526a",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"./figures/feature_selection.png\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecacad",
   "metadata": {},
   "source": [
    "Feature selection is the process of eliminating non-informative features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740624f",
   "metadata": {},
   "source": [
    "### The curse of dimensionality\n",
    "Not observing enough data to support a meaningful relationship.\n",
    "<img align=\"center\" src=\"./figures/curse_of_dimensionality.png\" width=\"800px\"/>\n",
    "<a href=\"https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/\">Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0f7daa",
   "metadata": {},
   "source": [
    "\n",
    "<img align=\"center\" src=\"figures/curse_of_dimensionality_boxes.png\" width=\"1200px\"/>\n",
    "As the number of features or dimensions grows, the amount of data we need to generalise accurately grows **exponentially** e.g $5^1$, $5^2$, $5^3$, $5^n$\n",
    "<a href=\"https://www.freecodecamp.org/news/the-curse-of-dimensionality-how-we-can-save-big-data-from-itself-d9fa0f872335/\">Source</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d76dbf",
   "metadata": {},
   "source": [
    "## Feature correlation\n",
    "One selection technique is to remove one (or more) of  features that are highly correlated to each other.\n",
    "* High correlation = redundant information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8d1e2",
   "metadata": {},
   "source": [
    "### üñ• Pearson Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32392e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=data.drop(columns=['Weight', 'Species'])\n",
    "y=data.Weight\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eabdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data = X_train.copy()\n",
    "corr_data['target'] = y_train\n",
    "corr = corr_data.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(1,1, figsize=(16,14))\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns,\n",
    "        cmap= \"seismic\",ax=ax, annot=corr);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112aede5",
   "metadata": {},
   "source": [
    "## Model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0476e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, cv=10)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b80de2",
   "metadata": {},
   "source": [
    "## Model with only best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "scores = cross_val_score(LinearRegression(), X_train[['Length3']], y_train, cv=10)\n",
    "\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d337a7",
   "metadata": {},
   "source": [
    "# Feature Permutation \n",
    "Feature permutation is a second feature selection algorithm that evaluates the importance of each feature in predicting the target.\n",
    "* Trains and records the test score of a base model containing all features \n",
    "* Randomly shuffles (permutation) <span style=\"color:blue\">**one**</span> feature within the test set \n",
    "* Records new score on shuffled test set \n",
    "* Compares the new score to the original score \n",
    "* Repeat for each feature \n",
    "üëâ <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html\">Sklearn's <code>permutation_importance</code> documentation</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64b0b3",
   "metadata": {},
   "source": [
    "üëâ If the score drops when a feature is shuffled, it is considered important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170c94c",
   "metadata": {},
   "source": [
    "### üíª Feature permutation in Sklearn\n",
    "\n",
    "This time, let's see if turning this problem into a classification problem works better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X=data.drop(columns=['Species'])\n",
    "y=data.Species\n",
    "\n",
    "encoder = LabelEncoder().fit(y)\n",
    "y_cat = encoder.transform(y)\n",
    "\n",
    "X_train, X_test, y_train_cat, y_test_cat = train_test_split(X, y_cat, train_size=.7, random_state=1)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "model.fit(X_train, y_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_score = permutation_importance(model, X_train, y_train_cat, n_repeats=10) # Perform Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259eed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame(np.vstack((X_train.columns,\n",
    "                                        permutation_score.importances_mean)).T) # Unstack results\n",
    "importance_df.columns=['feature','score decrease']\n",
    "\n",
    "importance_df.sort_values(by=\"score decrease\", ascending = False) # Order by importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d018b7a4",
   "metadata": {},
   "source": [
    "## Model with all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5804156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_model = LogisticRegression(max_iter=3000)\n",
    "\n",
    "scores = cross_val_score(base_model, X_train, y_train_cat, cv=3)\n",
    "\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9f4b9",
   "metadata": {},
   "source": [
    "## Model with best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffdd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_small = X_train[[\"Length3\",\"Width\", \"Height\"]] # Keep strong features\n",
    "\n",
    "final_model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "scores = cross_val_score(final_model, X_small, y_train_cat, cv=3)\n",
    "\n",
    "scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db9d557",
   "metadata": {},
   "source": [
    "### Reducing Complexity\n",
    "The most simple solution is normally the best solutionüî™\n",
    "Reducing the number of features makes the model:\n",
    "* More interpretable \n",
    "* Faster to train \n",
    "* Easier to implement and maintain in production "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3feec54",
   "metadata": {},
   "source": [
    "# Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4d9d4",
   "metadata": {},
   "source": [
    "Regularization means adding a **penalty term** to the Loss that **increases** with $\\beta$\n",
    "$$\\text{Regularized Loss} = Loss(X,y, \\beta) + Penalty(\\beta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1cb65",
   "metadata": {},
   "source": [
    "üëâ Penalizes large values for $\\beta_i$<br>\n",
    "üëâ Forces model to shrink certain coefficients or even select less features<br>\n",
    "üëâ Prevents overfitting <br>\n",
    "$$\\hat{y} =  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_1^3 + ... $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916b0792",
   "metadata": {},
   "source": [
    "Two famous Regularization penalties:<br><br>\n",
    "**Lasso** (L1)<br>\n",
    "$$L1 = Loss  + \\alpha \\sum_{i=1}^n |\\beta_i|$$ <br>\n",
    "**Ridge** (L2)<br>\n",
    "$$L2 = Loss + \\alpha \\sum_{i=1}^n \\beta_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2092f",
   "metadata": {},
   "source": [
    "Introduces the new hyper-parameter $\\alpha$:\n",
    "* Dictates **how much** the model is **regularized**\n",
    "* Large $\\alpha$ force **model complexity and variance to decrease**, but **bias increases**\n",
    "* Notice $\\sum$ starts from $i=1$, i.e. intercept coefficient is not penalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef1cf6",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è   Always **scale** your feature before regularization to penalize each $\\beta_i$ fairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cb3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "\n",
    "def test_reg_coefs(data=lc_train):\n",
    "    X = data[['Length1','Length2','Length3', 'Height']]\n",
    "\n",
    "    poly_tr = PolynomialFeatures(degree=2).fit(X)\n",
    "    X = pd.DataFrame(poly_tr.transform(X))\n",
    "    X = pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\n",
    "\n",
    "    # The target is the weight of a fish\n",
    "    Y = data[['Weight']] \n",
    "\n",
    "    linreg = LinearRegression().fit(X, Y)\n",
    "    ridge = Ridge(alpha=1.5, max_iter=2000).fit(X, Y)\n",
    "    lasso = Lasso(alpha=1.5, max_iter=2000).fit(X, Y)\n",
    "\n",
    "    coefs = pd.DataFrame({\n",
    "        \"coef_linreg\": pd.Series(linreg.coef_[0], index = X.columns),\n",
    "        \"coef_ridge\": pd.Series(ridge.coef_[0], index = X.columns),\n",
    "        \"coef_lasso\": pd.Series(lasso.coef_, index= X.columns)})\\\n",
    "\n",
    "    return coefs\\\n",
    "        .applymap(lambda x: int(x))\\\n",
    "        .style.applymap(lambda x: 'color: red' if x == 0 else 'color: black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e36a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reg_coefs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27821fc",
   "metadata": {},
   "source": [
    "# Model Tuning: Finding the best Hyperparameters\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_tuning.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>A jazz band playing a happy tune on their saxophone in the lively streets of New Orleans, vivid colors</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9682df",
   "metadata": {},
   "source": [
    "### The grid search method\n",
    "Explores different hyperparam value combinations to find those optimizing performance\n",
    "<img src=\"figures/grid_search.png\" style=\"width:600px;\">\n",
    "<a href=\"https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35\">Stalfort, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083c424",
   "metadata": {},
   "source": [
    "* Also applied using a *validation set* (never use test set for model tuning!)\n",
    "* Select which grid of values of hyper-parameters to try out\n",
    "* For each combinations of values, measure your performance on the *validation set*\n",
    "* Select hyperparams that produce the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ef55a",
   "metadata": {},
   "source": [
    "**üî• Grid Search CV**<br>\n",
    "<img src=\"figures/full_workflow_validation.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://stats.stackexchange.com/questions/424477/how-to-make-train-test-split-with-given-class-weights\">StackExchange, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c4fed9",
   "metadata": {},
   "source": [
    "### Sklearn  <code>GridSearchCV</code> üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526439b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop(columns=['Species', 'Weight'])\n",
    "y = data[['Weight']]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'alpha': [0.01, 0.1, 1], \n",
    "        'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg']}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = GridSearchCV(model, grid, \n",
    "                           scoring = 'r2',\n",
    "                           cv = 5,\n",
    "                           n_jobs=-1 # paralellize computation\n",
    "                          ) \n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6146ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best score\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13798458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Params\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448c00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best estimator\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9239a24",
   "metadata": {},
   "source": [
    "\n",
    "<p>üëé Limitations of Grid Search:</p>\n",
    "<ul>\n",
    "<li>Computationally costly</li>\n",
    "<li>The optimal hyperparameter value can be missed</li>\n",
    "<li>Can overfit hyperparameters to the training set if too many combinations are tried out for too small a dataset</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b023b6b",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a1d82",
   "metadata": {},
   "source": [
    "Randomly explore hyperparameter values from:\n",
    "* A hyperparameter space to randomly sample from\n",
    "* The specified number of samples to be tested<br>\n",
    "<img src=\"figures/grid_search2.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://medium.com/@jackstalfort/hyperparameter-tuning-using-grid-search-and-random-search-f8750a464b35\">Stalfort, 2019</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470f73f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy import stats\n",
    "\n",
    "# Instanciate model\n",
    "model = Ridge()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "grid = {'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg'], 'alpha': stats.loguniform(0.01,1)}\n",
    "\n",
    "# Instanciate Grid Search\n",
    "search = RandomizedSearchCV(model, grid, \n",
    "                            scoring='r2',\n",
    "                            n_iter=100,  # number of draws\n",
    "                            cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "search.fit(X_train, y_train)\n",
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17befd9c",
   "metadata": {},
   "source": [
    "### Using a probability distribution in RandomSearch\n",
    "Can be generated with <a href=\"https://docs.scipy.org/doc/scipy/reference/stats.html\">scipy.stats.distributions</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc9c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dist = stats.norm(10, 2) # if you have a best guess (say: 10)\n",
    "\n",
    "dist = stats.randint(1,100) # if you have no idea\n",
    "dist = stats.uniform(1, 100) # same\n",
    "\n",
    "dist = stats.loguniform(0.01, 1) # Coarse grain search\n",
    "\n",
    "r = dist.rvs(size=10000) # Random draws\n",
    "plt.hist(r);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50557e9a",
   "metadata": {},
   "source": [
    "### Limitations of GridSearch and RandomizedSearch:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f884fb",
   "metadata": {},
   "source": [
    "* Both algorithms are not tracking the history of optimization üìú"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b20f94",
   "metadata": {},
   "source": [
    "* Choice of next set of parameter is random üé≤ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39ba34",
   "metadata": {},
   "source": [
    "* Evaluation of the loss function is costly üí∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f98a2d",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "<p><img src=\"figures/bayes_theorem_visual.jpeg\" style=\"width:600px;\"></p>\n",
    "<a href=\"https://luminousmen.com/post/data-science-bayes-theorem\">Source: Bayes Theorem in Data Science</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c60e2",
   "metadata": {},
   "source": [
    "### Principle of Bayesian Optimization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce4849",
   "metadata": {},
   "source": [
    "* Build a surragate function that is quick to evaluate (prior) üé≠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894f885",
   "metadata": {},
   "source": [
    "* Select the next set of hyperparameters based on the surrogate function, evaluate them on the loss function ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f19fa",
   "metadata": {},
   "source": [
    "* Update the surroage function (posterior) üï∞Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90a334",
   "metadata": {},
   "source": [
    "<img src=\"figures/bayesOpt1.png\" style=\"width:800\">\n",
    "<a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">Koehrsen, 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44a6a99",
   "metadata": {},
   "source": [
    "<img src=\"figures/bayesOpt2.png\" style=\"width:800\">\n",
    "<a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">Koehrsen, 2018</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead96c29",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Implementation\n",
    "We can use BayesSearch <code>skopt</code> do do Bayesian Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81afda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "X = data.drop(columns=['Species', 'Weight'])\n",
    "y = data[['Weight']]\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=.3, random_state=0)\n",
    "\n",
    "#‚ö†Ô∏è Polynomial Transformation first ‚ö†Ô∏è\n",
    "poly_tr = PolynomialFeatures(degree=3).fit(X_train)\n",
    "X_train = poly_tr.transform(X_train)\n",
    "X_test = poly_tr.transform(X_test)\n",
    "    \n",
    "    \n",
    "#‚ö†Ô∏è Data must be centered around its mean before applying PCA ‚ö†Ô∏è\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# log-uniform: understand as search over p = exp(x) by varying x\n",
    "opt = BayesSearchCV(\n",
    "    Lasso(max_iter=1000),\n",
    "    {\n",
    "        'alpha': Real(0.1, 10, prior='log-uniform'),\n",
    "        'max_iter': [3000, 5000]\n",
    "    },\n",
    "    n_iter=32,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"val. score: %s\" % opt.best_score_)\n",
    "print(\"test score: %s\" % opt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340d957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_objective, plot_histogram\n",
    "\n",
    "_ = plot_objective(opt.optimizer_results_[0],\n",
    "                   dimensions=[\"alpha\", \"max_iter\"],\n",
    "                   n_minimum_search=int(1e8), size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650079b",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "<br>\n",
    "\n",
    "<center><img src=\"figures/DALLE_svm.png\" style=\"width:900px;\">\n",
    " ¬© C√©dric John, 2022; Image generated with <a href=\"https://openai.com/blog/dall-e/\">DALL-E</a>\n",
    "<br>Prompt: <i>Two Doric columns supporting a temple in a soft green light, digital art</i>.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc22501",
   "metadata": {},
   "source": [
    "### What is the optimal decision boundary for this classification?\n",
    "<img src=\"figures/SVM_planes.png\" style=\"width:1200px\">\n",
    "<a href=\"https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c\">Ippoloto, 2019</a>\n",
    "<p>Infinite number of potential decision boundaries that separate the classes (\"hyperplanes\")</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e023c",
   "metadata": {},
   "source": [
    "<img src=\"figures/SVM_margin.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c\">Ippoloto, 2019</a>\n",
    "\n",
    "* The hyperplane that generalizes best to unseen data is the one that is furthest from all the points (maximizes the **margin**)\n",
    "* The points on the margin boundary are called **support vectors**\n",
    "* Finding them is a convex optimization problem (one single best solution)\n",
    "* **Maximum Margin Classifier** algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fb305",
   "metadata": {},
   "source": [
    "* Max Margin is super sensitive to outliers\n",
    "* It **overfits** to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495a8fc",
   "metadata": {},
   "source": [
    "For **generalization** purpose, we may want to allow some points to be **inside** the margin, or even **on the other side** of the decision boundary:<br>\n",
    "<img src=\"figures/1_M_3iYollNTlz0PVn5udCBQ.png\" style=\"width:900px;\">\n",
    "<a href=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\">Mishra, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd541cd",
   "metadata": {},
   "source": [
    "### Soft margin classifier\n",
    "Allows a few points to be misclassified but with a **penalty ($\\xi$)** for how \"far\" they lie on the wrong side of the margin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930a4ac",
   "metadata": {},
   "source": [
    "The **Hinge Loss** is the penalty applied to each point on the wrong side<br>\n",
    "* The deeper a point lies within the margin, the higher the loss\n",
    "* The penalty is linear, like MAE <br>\n",
    "<img src=\"figures/1_M_3iYollNTlz0PVn5udCBQ.png\" style=\"width:800px;\">\n",
    "<a href=\"https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe\">Mishra, 2019</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693eb44",
   "metadata": {},
   "source": [
    "<img src=\"figures/hinge_loss_.png\" style=\"width:1300px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bfc0f1",
   "metadata": {},
   "source": [
    "### Regulariation hyperparameter <code>C</code>\n",
    "Stength of the penalty applied on points being on the wrong side of the margin\n",
    "* The higher <code>C</code>, the stricter the margin\n",
    "* A \"maximum margin classifier\" has <code>C</code> = $+ \\infty$\n",
    "* The smaller <code>C</code>, the softer the margin, the more it is ***regularized***\n",
    "* C similar to $1/ \\alpha$ in Ridge \n",
    "<img src=\"figures/svm_regularization.png\" style=\"width:1200px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f31e5d",
   "metadata": {},
   "source": [
    "\n",
    "<p>üíª sklearn implementation</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea25566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent but with SGD solver\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "svc_bis = SGDClassifier(loss='hinge', penalty='l2', alpha=1/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf745dc",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning: All support vector models requires **scaling**\n",
    "<img src=\"figures/svm_scaling.png\" style=\"width:1800px;\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d0fcad",
   "metadata": {},
   "source": [
    "# SVM 'kernels'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bd8a4",
   "metadata": {},
   "source": [
    "<img src=\"figures/SVM_nonlin.png\" style=\"width:1200px\">\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d1b698",
   "metadata": {},
   "source": [
    "## We can create more features to separate this data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b49cab",
   "metadata": {},
   "source": [
    "<img src=\"figures/SVM_kernel1.png\" style=\"width:1300px\">\n",
    "\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954036eb",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Creates new features - curse of dimensionality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570fe8c",
   "metadata": {},
   "source": [
    "üåΩ Instead, we can use a mathematical <strong style=\"color:teal\">'kernel'</strong> to simulate new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779e3aa",
   "metadata": {},
   "source": [
    "# The kernel-trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4724c0",
   "metadata": {},
   "source": [
    "üìè Measure a distance pair-wise between each sample and use this to simulate creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d982c",
   "metadata": {},
   "source": [
    "* ***Linear kernel function*** for linear datasets (the best for high-dimensional datasets - speed!)<br>$F(x, xj) = sum( x.xj)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f39837",
   "metadata": {},
   "source": [
    "* ***Polynomial kernel functions*** (example of previous slide with $X^3$)<br> $F(x, xj) = (x.xj+1)^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a31e2b",
   "metadata": {},
   "source": [
    "* ***Gaussian Radial Basis function kernel (RBF)*** (one of the favourite kernels for non-linear datasets)<br> $F(x, xj) = \\exp(-\\gamma * ||x - xj||^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196e501",
   "metadata": {},
   "source": [
    "* ***Sigmoid kernel function*** <br> $F(x, xj) = tanh(Œ±xay + c)$<br><br><a href=\"https://dataaspirant.com/svm-kernels/\">Good reference on SVMs with kernels</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc6574d",
   "metadata": {},
   "source": [
    "## SVM-Regressors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1143c6bf",
   "metadata": {},
   "source": [
    "Reverse the objective:\n",
    "* **Classification**: fit the largest possible *street* **between** two classes\n",
    "* **Regression**: fit as many points as possible **within** the *street*\n",
    "* Width of the street controlled by an additional hyperparam $\\epsilon$\n",
    "<img src=\"figures/svm_regressor.png\" style=\"width:1200px\">\n",
    "<a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">Geron, 2017</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b525903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(epsilon=1, kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6071e4c6",
   "metadata": {},
   "source": [
    "# Suggested Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf765cd",
   "metadata": {},
   "source": [
    "## üì∫ Videos \n",
    "#### Short videos from my Undegraduate Machine Learning Classes:\n",
    "* üìº <a href=\"https://youtu.be/8mNPHGmXS5Q?list=PLZzjCZ3QdgQCcRIwQdd-_cJNAUgiEBB_n\">Support Vector Machines</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d32391",
   "metadata": {},
   "source": [
    "## üìö Further Reading \n",
    "* üìñ <a href=\"https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47\">Support Vector Machine ‚Äî Introduction to Machine Learning Algorithms</a> by Rohith Gandhi, 2018\n",
    "* üìñ <a href=\"https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf\">Taking the Human Out of the Loop: A Review of Bayesian Optimization</a> by Shariari et al\n",
    "* üìñ <a href=\"https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a> by Will Koehrsen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156d3a2",
   "metadata": {},
   "source": [
    "## üíªüêç Time to Code ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
